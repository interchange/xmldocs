User &glos-session;s in Interchange are usually kept as files in the 
<filename class='directory'>session/</filename> directory (or inside
a DBM database) for each &glos-catalog;. Since session
data is not deleted after sessions end (or timeout), periodic expiring
needs to be set up to keep the session database or session files from growing
too large, wasting disk space and slowing down directory lookups.
</para><para>
There's no worry that expiring will do any harm, because all our scripts
only clean up unused sessions. Active users will not notice any change.
</para><para>
The simplest way to expire catalog's session files is to run
<command>expire -c <replaceable>CATALOG_NAME</replaceable></command>.
</para><para>
For convenience, there is also <command>expireall</command> script which
reads all catalog entries in &gcf; and runs <command>expire</command> on them.
</para><para>
The <command>expire</command> script accepts a <literal>-r</literal> option
which, when DBM sessions are used, tells the script to reorganize database
files and recover lost disk space.
</para><para>
On a UNIX server, it's most useful to run <command>expireall</command>
from <literal>crontab</literal>. As the &IC; user, run
<command>crontab -e</command> to edit crontab data, and enter something like:
<programlisting>
# once a day at 4:40 am
40 4 * * *    /PATH/TO/perl /PATH/TO/INTERCHANGE/bin/expireall -r
</programlisting>
</para>
<note>
<para>
If a session saved search paging files in &conf-ScratchDir;, they would 
not be deleted (see below for a solution).
</para>
</note>

<para>
When file-based sessions are used (no DBM), then you can use a custom
script like this:

<!-- TODO how come we call -f and then -d. As far as I know, both can't
succeed for the same file. Also include a note on cleaning tmp/ and 
the find | xargs lines -->

<programlisting><![CDATA[
#!perl
# expire_sessions.pl -- delete files 2 days old or older
# invoke as: /PATH/TO/perl expire_sessions.pl /PATH/TO/CATALOG/session/ ...

my @files;
my $dir;
foreach $dir (@ARGV) {
	 # just push files on the list
	 if (-f $dir) { push @files, $_; next; }

	 next unless -d $dir;

	 # get all the file names in the directory
	 opendir DIR, $dir or die "opendir $dir: $!\n";
	 push @files, ( map { "$dir/$_" } grep(! /^\.\.?$/, readdir DIR));
}

for (@files) {
	 unless (-f $_) {
			 warn "skipping $_, not a file.\n";
			 next;
	 }
	 next unless -M $_ >= 2;
	 unlink $_ or die "unlink $_: $!\n";
}
]]></programlisting>
This script can be adjusted as necessary. Refinements might include reading
the file to "eval" the session reference and expire only customers who are not 
registered members.
</para><para>
If your files get chown-ed to root every day, then you probably used root's
instead of &IC; user's crontab file. Either move the crontab to the 
&IC; user, or use <command>su</command> to swith users from the root
account:
<programlisting>
44 4 * * * su -c "/PATH/TO/INTERCHANGE/bin/expireall -r" IC_USERNAME
</programlisting>

The above does not, however, clean temporary files from the &conf-ScratchDir; 
directory. We don't often use the expire scripts any more. We just use
a small standalone script <filename>clean_session_tmp</filename>:
<programlisting><![CDATA[
#!/bin/sh

for DIR in $*; do
  for i in session tmp; do
    if test -d "$DIR/$i"; then
      find $DIR/$i -type f -mmin +480 | xargs --no-run-if-empty rm
      find $DIR/$i -type d -empty -mtime +2 -depth | xargs --no-run-if-empty rmdir
    else
      echo "$0: $DIR/$i doesn't exist.";
    fi
  done
done
]]></programlisting>
using a cron entry similar to:
<programlisting>
44 0,4,8,12,16,20 * * * <replaceable>DIR/bin/</replaceable>clean_session_tmp <replaceable>/path/to/catdir1</replaceable> <replaceable>/path/to/catdir2</replaceable>
</programlisting>

