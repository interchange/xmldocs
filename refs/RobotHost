__NAME__ purpose
specify hostnames that will be classified as crawler bots (search engines) visiting the site
__END__


__NAME__ see also
RobotUA, RobotIP
__END__

__NAME__ synopsis
<group choice='req'>
	<arg choice='plain' rep='repeat'><replaceable>hostname</replaceable></arg>
</group>
__END__


__NAME__ description
The &conf-RobotHost; directive defines a list of hostnames which will be
classified as crawler robots (search engines), and cause &IC; to alter its
behavior to improve the chance of &IC;-served content being crawled
and indexed.
</para><para>
Note that this directive (and all other work done to identify robots)
only serves to improve the way in which &IC; pages are indexed, and to 
reduce server overhead for clients that don't require our full attention
in the way humans do (for example, session information is not kept around
for spider bots).
Using this to "tune" the actual page content depending on a crawler
visiting does not earn you extra points, and may in fact be
detected by the robot and punished.
</para><para>
It's important to note that the directive accepts a wildcard list -
<literal>*</literal> represents any number of characters, while
<literal>?</literal> represents a single character.
__END__

__NAME__ notes
For more details regarding web spiders/bots and &IC; see
&glos-robot; glossary entry.
__END__

__NAME__ example: Defining RobotHost
<programlisting><![CDATA[
RobotHost <<EOR
  *.crawler*.com,     *.excite.com,           *.googlebot.com,
  *.infoseek.com,     *.inktomi.com,          *.inktomisearch.com,
  *.lycos.com,        *.pa-x.dec.com,         add-url.altavista.com,
  westinghouse-rsl-com-usa.NorthRoyalton.cw.net,
EOR
]]></programlisting>
__END__

